{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rawpy\n",
    "import imageio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from spectral import *\n",
    "import spectral.io.aviris as aviris\n",
    "import matplotlib\n",
    "matplotlib.use('WXAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import wx\n",
    "from skimage.restoration import (denoise_wavelet, estimate_sigma)\n",
    "from scipy.signal import argrelextrema\n",
    "import scipy\n",
    "import cv2\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import cv2\n",
    "spectral.settings.WX_GL_DEPTH_SIZE = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Convolution1D, Dropout, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.initializers import random_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 2048\n",
    "band = 98\n",
    "\n",
    "lambda_list = [\n",
    " 601.00,\n",
    " 602.00,\n",
    " 606.00,\n",
    " 610.00,\n",
    " 612.36,\n",
    " 616.73,\n",
    " 621.20,\n",
    " 624.07,\n",
    " 626.71,\n",
    " 628.05,\n",
    " 631.63,\n",
    " 636.59,\n",
    " 641.24,\n",
    " 646.58,\n",
    " 651.24,\n",
    " 654.85,\n",
    " 658.63,\n",
    " 660.78,\n",
    " 665.41,\n",
    " 669.57,\n",
    " 673.58,\n",
    " 683.94,\n",
    " 688.45,\n",
    " 692.56,\n",
    " 696.90,\n",
    " 699.05,\n",
    " 703.90,\n",
    " 708.03,\n",
    " 712.41,\n",
    " 719.38,\n",
    " 724.03,\n",
    " 727.76,\n",
    " 731.65,\n",
    " 734.10,\n",
    " 737.94,\n",
    " 741.27,\n",
    " 744.62,\n",
    " 752.29,\n",
    " 758.12,\n",
    " 762.98,\n",
    " 767.84,\n",
    " 770.30,\n",
    " 775.42,\n",
    " 780.03,\n",
    " 784.55,\n",
    " 790.34,\n",
    " 795.45,\n",
    " 799.87,\n",
    " 804.17,\n",
    " 806.41,\n",
    " 810.84,\n",
    " 814.31,\n",
    " 817.33,\n",
    " 827.88,\n",
    " 832.47,\n",
    " 836.32,\n",
    " 840.31,\n",
    " 842.50,\n",
    " 846.97,\n",
    " 850.85,\n",
    " 854.61,\n",
    " 859.38,\n",
    " 863.32,\n",
    " 866.84,\n",
    " 870.24,\n",
    " 872.16,\n",
    " 875.86,\n",
    " 879.01,\n",
    " 881.74,\n",
    " 886.01,\n",
    " 889.95,\n",
    " 893.54,\n",
    " 897.35,\n",
    " 899.58,\n",
    " 903.55,\n",
    " 906.80,\n",
    " 909.86,\n",
    " 914.24,\n",
    " 917.38,\n",
    " 920.24,\n",
    " 923.17,\n",
    " 924.90,\n",
    " 928.03,\n",
    " 930.66,\n",
    " 933.14,\n",
    " 942.38,\n",
    " 945.32,\n",
    " 947.88,\n",
    " 950.70,\n",
    " 952.39,\n",
    " 955.99,\n",
    " 959.30,\n",
    " 962.35,\n",
    " 966.54,\n",
    " 969.13,\n",
    " 971.34,\n",
    " 973.58,\n",
    " 975.08,\n",
    "]\n",
    "\n",
    "\n",
    "dic_wave_length = {}\n",
    "for k in range(band):\n",
    "    dic_wave_length[k] = lambda_list[k] \n",
    "\n",
    "# for key, value in dic_wave_length.items():\n",
    "#     if value<701 and value >699:\n",
    "#         print((key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_cube(path, w=w, band=band):\n",
    "    data_before_process = np.fromfile(path, dtype=np.uint16)\n",
    "    # data_before_process = np.fromfile(path)\n",
    "    h = int(len(data_before_process)/(w*band))\n",
    "\n",
    "    data_1 = np.reshape(data_before_process, (h, w*band))\n",
    "    \n",
    "    cube = np.zeros((h, w, band), dtype = np.uint16)\n",
    "    for k in range(h):\n",
    "        cube[k,:,:] = np.transpose(np.reshape(data_1[k,:], (band, w)))\n",
    "    return cube\n",
    "\n",
    "def background_removal(cube_of_interest):\n",
    "    h = int((np.shape(cube_of_interest)[0]))\n",
    "    cropped_cube = np.zeros((h,w,band))\n",
    "    vi = ndvi(cube_of_interest, 25, 49)\n",
    "    vi_mask = np.where(vi < 0.3, 0, 1)\n",
    "    for k in range(band):\n",
    "        cropped_cube[:,:,k] = np.multiply(cube_of_interest[:,:,k],vi_mask)\n",
    "    return cropped_cube\n",
    "\n",
    "def find_interesting_band_with_successive_var(cropped_cube_of_interest): \n",
    "    var_list = []\n",
    "    for k in range(band):\n",
    "        var_list.append(np.var(cropped_cube_of_interest[:,:,k]))\n",
    "    dvar = np.gradient(var_list, 1)\n",
    "    print(argrelextrema(dvar, np.greater))\n",
    "    return dvar\n",
    "\n",
    "def create_smooth_cube(cube_of_interest):\n",
    "    dim = np.shape(cube_of_interest)\n",
    "    smoothed_cube = np.zeros((dim[0], dim[1], dim[2]), dtype=np.uint8)\n",
    "    # for i in range(dim[0]):\n",
    "    #     for j in range(dim[1]): \n",
    "    #         smoothed_cube[i,j,:] = scipy.signal.savgol_filter(cropped_cropped_cube[i,j,:], deriv = 1, polyorder = 3, window_length = 29)\n",
    "    \n",
    "    smoothed_cube[:,:,:] = scipy.signal.savgol_filter(cube_of_interest[:,:,:], axis = 2, deriv = 0, polyorder = 3, window_length= 9)\n",
    "    return smoothed_cube\n",
    "\n",
    "def create_final_cube (path):\n",
    "    cube = create_raw_cube(path)\n",
    "    cropped_cube = background_removal(cube)[:,250:1750,:]\n",
    "    smoothed_cube = create_smooth_cube(cropped_cube)\n",
    "    return smoothed_cube\n",
    "\n",
    "def signaltonoise(a, axis=0, ddof=0):\n",
    "    vect = np.matrix.flatten(a)\n",
    "    m = np.mean(vect)\n",
    "    sd = np.std(vect)\n",
    "    return np.where(sd == 0, 0, m/sd)\n",
    "\n",
    "def denoise_image(cube_of_interest):\n",
    "    dim = np.shape(cube_of_interest)\n",
    "    print(dim)\n",
    "    denoised_image = np.zeros((dim[0], dim[1], dim[2]), dtype='uint8')\n",
    "    for k in range(dim[2]):\n",
    "        slice = cube_of_interest[:,:,k]\n",
    "        index = np.nonzero(slice)\n",
    "        created_slice = np.zeros((dim[0], dim[1]))\n",
    "        created_slice[index] = denoise_wavelet(slice[index], channel_axis = None,\n",
    "                            method='BayesShrink', mode='soft',\n",
    "                            rescale_sigma=True)\n",
    "        # sliceCopy = np.uint8(slice)\n",
    "        # for k in range(len(index[0])):\n",
    "        #     created_slice[index[0][k]][index[1][k]] = cv2.fastNlMeansDenoising(sliceCopy[index])\n",
    "        denoised_image[:,:,k] = created_slice\n",
    "    return denoised_image\n",
    "\n",
    "def create_LLSI_and_CCTR1_images(cube_of_interest):\n",
    "    LLSI = np.divide(cube_of_interest[:,:,29]-cube_of_interest[:,:,0], cube_of_interest[:,:,29]+cube_of_interest[:,:,0]) - cube_of_interest[:,:,54]\n",
    "    CTR1 = np.divide(cube_of_interest[:,:,25], cube_of_interest[:,:,0])\n",
    "\n",
    "    \n",
    "    arr8 = ((LLSI-np.mean(LLSI))/np.var(LLSI)*256).astype(np.uint8)\n",
    "    # print(np.shape(LLSI))\n",
    "    heatmap = cv2.applyColorMap(arr8, cv2.COLORMAP_RAINBOW)\n",
    "    imshow( heatmap)\n",
    "    imshow(LLSI)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cube creation (background removal, crop the edges and spectral smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_YR_1 = create_final_cube('Vuka_YR_31_days_low/image_1.raw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_YR_2 = create_final_cube('Vuka_YR_31_days_low/image_2.raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cube_YR_3 = create_final_cube('vuka_yellowrust_multileaves_1_1.raw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_healthy_1 = create_final_cube('Vuka Healthy 31 days/image_1.raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to remove the noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoise_cube_YR_1 = denoise_image(cube_YR_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_list = []\n",
    "for k in range(band):\n",
    "    slice = cube_YR_1[:,:,k]\n",
    "    index = np.nonzero(slice)\n",
    "    snr_list.append(signaltonoise(slice[index]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_list_denoise = []\n",
    "for k in range(band):\n",
    "    slice = denoise_cube_YR_1[:,:,k]\n",
    "    index = np.nonzero(slice)\n",
    "    snr_list_denoise.append(signaltonoise(slice[index]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research of bands of interest (with ttest first, and then with pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_list = []\n",
    "for k in range(band):\n",
    "    slice_1 = cube_healthy_1[:,:,k]\n",
    "    slice_2 = cube_YR_1[:,:,k]\n",
    "    index_1 = np.nonzero(slice_1)\n",
    "    index_2 = np.nonzero(slice_2)\n",
    "    ttest_list.append(scipy.stats.ttest_ind(slice_1[index_1], slice_2[index_2])[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1bbf17dd2e0>]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_max_ttest = argrelextrema(np.array(ttest_list), np.greater)\n",
    "plt.figure()\n",
    "plt.plot(lambda_list, ttest_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = principal_components(cube_YR_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_contribution_pca = []\n",
    "for i in range(band):\n",
    "    list_contribution_pca.append(pca.cov[i,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x17980609d30>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_ttest = (ttest_list + 500*np.ones(98))*10\n",
    "plt.figure()\n",
    "plt.plot(lambda_list, list_contribution_pca, label = \"variance selection (PCA)\")\n",
    "plt.plot(lambda_list, adjusted_ttest, label = \"t test selection\")\n",
    "plt.legend()\n",
    "# argrelextrema(np.array(list_contribution), np.greater)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x179965a6eb0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(lambda_list, list_contribution_pca, label = \"variance selection (PCA)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract pixels from the image, and give a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(cube_of_interest, label_of_interest = 0):\n",
    "    ## take the non zeros pixels and normalise with features\n",
    "    idx = np.nonzero(cube_of_interest[:,:,0])\n",
    "    non_zero_pixels = np.array(cube_of_interest)[idx[0], idx[1], :]\n",
    "    non_zero_pixels_band_reduced = non_zero_pixels[:,[45,53,66,75,96]]\n",
    "    ## size = (nb of pixels, 5)\n",
    "    normalized_cube = preprocessing.normalize((non_zero_pixels_band_reduced), axis =0)\n",
    "    labels = np.ones((len(idx[0])))*label_of_interest\n",
    "    return normalized_cube, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_batch(X_all, Y_all, batch_size, train = True):\n",
    "    if train:\n",
    "        \n",
    "        nb_YR = np.count_nonzero(Y_all)\n",
    "        nb_tot = len(X_all)\n",
    "        nb_healthy = nb_tot - nb_YR\n",
    "        random_index = random.sample(range(0,nb_healthy), int(batch_size/2)) + random.sample(range(nb_healthy,nb_tot), int(batch_size/2))\n",
    "        X = np.zeros((batch_size, 5))\n",
    "        X[:,:] = X_all[[random_index],:]\n",
    "        Y_list = np.array(Y_all)[[random_index]]\n",
    "        Y = np.asarray(Y_list).astype('float32').reshape((-1,1))\n",
    "        \n",
    "        return X, Y, random_index\n",
    "    else: \n",
    "        X = np.zeros((batch_size, 5))\n",
    "        random_index = random.sample(range(0,len(X_all)), int(batch_size))\n",
    "        X[:,:] = X_all[[random_index],:]\n",
    "        return X, random_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of X_train and Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_healthy, Y_train_healthy = create_data(cube_healthy_1, 0)\n",
    "X_train_YR, Y_train_YR = create_data(cube_YR_1, 1)\n",
    "\n",
    "\n",
    "X_train_all = np.concatenate((X_train_healthy, X_train_YR))\n",
    "Y_train_all = np.concatenate((Y_train_healthy, Y_train_YR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stock variables in an external file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# variables = [X_train_raw, Y_train_raw, cube_healthy_1, cube_YR_1]\n",
    "# import pickle\n",
    "# Savingvariables = open(\"data.txt\",\"wb\")\n",
    "# pickle.dump(variables, Savingvariables)\n",
    "# Savingvariables.close()\n",
    "\n",
    "## if I want to take them back\n",
    "# fichierini = \"data.txt\"\n",
    "# fichierSauvegarde = open(fichierini,\"rb\")\n",
    "# variables = pickle.load(fichierSauvegarde)\n",
    "# X_train_raw = variables[0]\n",
    "# Y_train_raw = variables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a smaller training set (computational purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manon\\AppData\\Local\\Temp/ipykernel_8852/248312806.py:10: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  Y_list = np.array(Y_all)[[random_index]]\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, random_index_train = extract_batch(X_train_all, Y_train_all, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_all = create_data(cube_YR_3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, random_index_test = extract_batch(X_test_all, [], 10000, train = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# model.add(Convolution1D(nb_filter=32, filter_length=3, input_shape= (32, len(X_train), 5), activation='relu'))\n",
    "\n",
    "### 10 000 samples, batch size = 32, 5 wavelengths\n",
    "model.add(Convolution1D(filters=32, kernel_size =3, input_shape= (5, 1), activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution1D(filters=16, kernel_size=1, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manon\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "opt = SGD(lr=0.000001)\n",
    "model.compile(loss='binary_crossentropy', optimizer= opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manon\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4526: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "79/79 [==============================] - 3s 30ms/step - loss: 0.6931 - acc: 0.4736\n",
      "Epoch 2/5\n",
      "79/79 [==============================] - 2s 26ms/step - loss: 0.6931 - acc: 0.4819\n",
      "Epoch 3/5\n",
      "79/79 [==============================] - 2s 21ms/step - loss: 0.6931 - acc: 0.4715\n",
      "Epoch 4/5\n",
      "79/79 [==============================] - 2s 26ms/step - loss: 0.6931 - acc: 0.4744\n",
      "Epoch 5/5\n",
      "79/79 [==============================] - 3s 33ms/step - loss: 0.6931 - acc: 0.4800: 1s - loss\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x231a563ca30>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8852/1564764984.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mY_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "Y_predicted = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert probability into class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manon\\AppData\\Local\\Temp/ipykernel_1216/2484113428.py:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  accuracy_score(Y_predicted[:,:,0], Y_train.astype(np.int))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4972"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_predicted = 1*(Y_predicted>0.5)\n",
    "\n",
    "\n",
    "accuracy_score(Y_predicted[:,:,0], Y_train.astype(np.int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manon\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4526: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Y_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_binary = (Y_test > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_map(cube_of_interest, random_index, prediction):\n",
    "    idx = np.transpose(np.nonzero(cube_of_interest[:,:,0]))\n",
    "    classe = np.ones(np.shape(cube_of_interest[:,:,0]))*2\n",
    "    j=0\n",
    "    for k in random_index:\n",
    "        classe[idx[k][0],idx[k][1]] = prediction[j]\n",
    "        j+=1\n",
    "    return(classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = imshow(cube_YR_3, (20,10,5), classes = classe_test)\n",
    "view.set_display_mode('overlay')\n",
    "view.class_alpha = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with a SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manon\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_SVM = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "classe_SVM = create_class_map(cube_YR_3, random_index_test, prediction_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageView object:\n",
       "  Display bands       :  [0, 49.0, 97]\n",
       "  Interpolation       :  <default>\n",
       "  RGB data limits     :\n",
       "    R: [0.0, 234.0]\n",
       "    G: [0.0, 255.0]\n",
       "    B: [0.0, 255.0]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imshow(cube_YR_3, classes = np.where(classe_SVM ==2, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1982000)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(np.where(classe_SVM == 2)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e5182107df0f1f4639caeff78abbbc015a0abe644d9cae996b8e04c4b389655"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
